# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pt2Y632wSV5Iqio4dFFO5KgoeS8NFYrQ
"""

#- Input: the unit_aggregates.csv must contain per‑unit TBDeff values for each channel
#and the geological parameters used in Table 3.
#- Computation: for each channel–parameter pair the script computes Pearson r,
#bootstrap 95% CI, and a two‑sided bootstrap p‑value; it records the sign and attaches the interpretation text.
#- Output: table3_summary.csv (numeric results) and table3_markdown.md
#(manuscript table matching the format you provided).
#- Reproducibility: N_BOOT and RANDOM_SEED are explicit; include the script and
#seed in your archive and cite them in Methods (Section 3.6).


#This script computes Pearson correlations between per‑channel TBDeff values (3, 7, 19, 37GHz)
#and auxiliary geological parameters. It bootstraps confidence intervals and p-values, records correlation signs,
#and attaches geological interpretations. The outputs correspond directly to Table3 in the manuscript.


#ANNOTATED WORKFLOW

# User parameters
INPUT_CSV = "unit_aggregates.csv"
UNIT_COL = "UNIT_ID"

# Channel columns to test
CHANNEL_COLS = {
    "3": "TBDeff_3GHz",
    "7": "TBDeff_7GHz",
    "19": "TBDeff_19GHz",
    "37": "TBDeff_37GHz"
}

# Parameters to test correlations against each channel
PARAMETERS = {
    "Hydrogen_ppm": "Hydrogen content",
    "OMAT": "Optical maturity (OMAT)",
    ...
}

# Interpretations for each parameter
INTERPRETATIONS = {
    "Hydrogen content": "Fine, cooler mantles reduce TBDeff amplitude",
    ...
}

# Bootstrap settings
N_BOOT = 10000
RANDOM_SEED = 20251201
np.random.seed(RANDOM_SEED)

#RAW CODE

"""
make_table3.py

Inputs:
 - CSV with one row per mapping unit containing:
    UNIT_ID,
    TBDeff_3GHz, TBDeff_7GHz, TBDeff_19GHz, TBDeff_37GHz,
    TiO2_wt, FeO_wt, Bolometric_T_anom, Regolith_T_anom,
    Plagioclase_wt, Hydrogen_ppm, Crustal_thickness_km,
    FreeAirGravity_mGal, OMAT
Outputs:
 - table3_summary.csv : numeric results with CI and p-values
 - table3_markdown.md  : manuscript-ready table (Parameter, Corr., Interpretation)
Dependencies:
 pip install numpy pandas scipy
"""
import json
import numpy as np
import pandas as pd
from scipy import stats

# ---------------- USER PARAMETERS ----------------
INPUT_CSV = "unit_aggregates.csv"   # edit to your aggregated CSV
UNIT_COL = "UNIT_ID"
CHANNEL_COLS = {
    "3": "TBDeff_3GHz",
    "7": "TBDeff_7GHz",
    "19": "TBDeff_19GHz",
    "37": "TBDeff_37GHz"
}

# Parameters to test (column_name: display_label)
PARAMETERS = {
    "Hydrogen_ppm": "Hydrogen content",
    "OMAT": "Optical maturity (OMAT)",
    "Plagioclase_wt": "Plagioclase abundance",
    "Bolometric_T_anom": "Bolometric T an.",
    "Regolith_T_anom": "Regolith temperature an.",
    "TiO2_wt": "TiO2 abundance",
    "FeO_wt": "FeO abundance",
    "Crustal_thickness_km": "Crustal thickness (Model 3)",
    "FreeAirGravity_mGal": "GRAIL free-air gravity"
}

# Interpretations mapping (customize as needed)
INTERPRETATIONS = {
    "Hydrogen content": "Fine, cooler mantles reduce TBDeff amplitude",
    "Optical maturity (OMAT)": "Ejecta mantling and lower maturity dampen excursions",
    "Plagioclase abundance": "Feldspathic veneers suppress the skin-layer response",
    "Bolometric T an.": "Radiative control peaks at the surface",
    "Regolith temperature an.": "Strongest alignment with skin-layer amplification",
    "TiO2 abundance": "High-Ti basalts maintain surface responsiveness",
    "FeO abundance": "Basaltic composition enhances radiative response",
    "Crustal thickness (Model 3)": "Structural suppression extends upward / thick crust correlates with suppression",
    "GRAIL free-air gravity": "Negative anomalies mark consolidated feldspathic terrains"
}

# Bootstrap settings
N_BOOT = 10000
RANDOM_SEED = 20251201
np.random.seed(RANDOM_SEED)

# ---------------- Load data ----------------
df = pd.read_csv(INPUT_CSV)
# Keep only rows with at least one channel value
required_cols = [UNIT_COL] + list(CHANNEL_COLS.values())
df = df.dropna(subset=[c for c in CHANNEL_COLS.values()])
n_units = len(df)
if n_units == 0:
    raise SystemExit("No units with channel TBDeff values found in input CSV.")

# ---------------- Helper: Pearson + bootstrap ----------------
def pearson_bootstrap(x, y, n_boot=N_BOOT):
    """Return observed r, bootstrap CI (2.5,97.5), and two-sided bootstrap p-value."""
    mask = ~np.isnan(x) & ~np.isnan(y)
    x0 = x[mask]; y0 = y[mask]
    if len(x0) < 3:
        return np.nan, np.nan, np.nan, np.nan
    obs_r, _ = stats.pearsonr(x0, y0)
    boot_r = np.empty(n_boot)
    n = len(x0)
    for i in range(n_boot):
        idx = np.random.choice(n, size=n, replace=True)
        try:
            r, _ = stats.pearsonr(x0[idx], y0[idx])
        except Exception:
            r = np.nan
        boot_r[i] = r
    ci = np.nanpercentile(boot_r, [2.5, 97.5])
    p_boot = np.mean(np.sign(boot_r) != np.sign(obs_r))
    return float(obs_r), float(ci[0]), float(ci[1]), float(p_boot)

# ---------------- Compute correlations per channel x parameter ----------------
rows = []
for ch_label, ch_col in CHANNEL_COLS.items():
    if ch_col not in df.columns:
        print(f"Warning: channel column '{ch_col}' not found; skipping channel {ch_label} GHz.")
        continue
    ch_vals = df[ch_col].values
    for param_col, param_label in PARAMETERS.items():
        if param_col not in df.columns:
            print(f"Warning: parameter column '{param_col}' not found; skipping {param_label}.")
            continue
        param_vals = df[param_col].values
        obs_r, ci_lo, ci_hi, p_boot = pearson_bootstrap(ch_vals, param_vals)
        sign = "+" if obs_r > 0 else ("-" if obs_r < 0 else "0")
        interp = INTERPRETATIONS.get(param_label, "")
        rows.append({
            "Channel_GHz": int(ch_label),
            "Parameter": param_label,
            "Pearson_r": obs_r,
            "r_ci_2.5": ci_lo,
            "r_ci_97.5": ci_hi,
            "Bootstrap_p_two_sided": p_boot,
            "Sign": sign,
            "Interpretation": interp
        })

out_df = pd.DataFrame(rows)
out_df = out_df.sort_values(["Channel_GHz", "Parameter"])

# ---------------- Save outputs ----------------
out_df.to_csv("table3_summary.csv", index=False)

# Markdown table for manuscript (grouped by channel)
md_lines = []
md_lines.append("| Ch. GHz | Parameter | Corr. | Interpretation |")
md_lines.append("|---:|---|:---:|---|")
for ch in sorted(out_df["Channel_GHz"].unique(), reverse=True):
    sub = out_df[out_df["Channel_GHz"] == ch]
    for _, r in sub.iterrows():
        md_lines.append(f"| {int(r['Channel_GHz'])} | {r['Parameter']} | {r['Sign']} | {r['Interpretation']} |")

with open("table3_markdown.md", "w") as fh:
    fh.write("\n".join(md_lines))

print("Table 3 summary written to: table3_summary.csv and table3_markdown.md")